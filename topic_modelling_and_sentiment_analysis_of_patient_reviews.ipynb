{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Installations\n",
        "!pip install bertopic\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "rhsfB3RVRg7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW-0FMMsMze9"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, pipeline\n",
        "from google.colab import drive\n",
        "from nltk.probability import FreqDist\n",
        "from bertopic import BERTopic\n",
        "from tabulate import tabulate\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a_RQqKbRe7v"
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf9Ui9sZb_9k"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive so Colab can access your files\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-5-g2Luc7VE"
      },
      "outputs": [],
      "source": [
        "# Define the full paths to the CSV files\n",
        "google_path = '/content/drive/My Drive/Colab Notebooks/google_reviews_synthetic.csv'\n",
        "trustpilot_path = '/content/drive/My Drive/Colab Notebooks/trustpilot_reviews_synthetic.csv'\n",
        "\n",
        "# Load the reviews CSV files into DataFrames\n",
        "google_df = pd.read_csv(google_path)\n",
        "trustpilot_df = pd.read_csv(trustpilot_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-6VL1r5qpP0"
      },
      "outputs": [],
      "source": [
        "print(google_df.columns)\n",
        "print(trustpilot_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_B3MxmdneQNC"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing review text\n",
        "google_df = google_df.dropna(subset=['Comment'])\n",
        "trustpilot_df = trustpilot_df.dropna(subset=['Review Content'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of unique locations in the Google data set using \"Club's Name\"\n",
        "num_unique_google_locations = google_df[\"Club's Name\"].nunique()\n",
        "print(f\"Number of unique locations in the Google data set: {num_unique_google_locations}\")\n",
        "\n",
        "# Find the number of unique locations in the Trustpilot data set using \"Location Name\"\n",
        "num_unique_trustpilot_locations = trustpilot_df[\"Location Name\"].nunique()\n",
        "print(f\"Number of unique locations in the Trustpilot data set: {num_unique_trustpilot_locations}\")"
      ],
      "metadata": {
        "id": "am1A4GGpjiMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the names by stripping whitespace and converting to lowercase\n",
        "google_locations = set(google_df[\"Club's Name\"].str.strip().str.lower().unique())\n",
        "trustpilot_locations = set(trustpilot_df[\"Location Name\"].str.strip().str.lower().unique())\n",
        "\n",
        "# Find common locations\n",
        "common_locations = google_locations.intersection(trustpilot_locations)\n",
        "\n",
        "# Output the number of common locations\n",
        "print(f\"Number of common locations between Google and Trustpilot: {len(common_locations)}\")"
      ],
      "metadata": {
        "id": "DuoPeYP_muvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkOXhQmHem8G"
      },
      "outputs": [],
      "source": [
        "# Define the stopwords (from NLTK)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Custom stopwords you want to remove manually\n",
        "custom_stopwords = {'get', 'one', 'always', 'also', 'dont', 'even', 'like', 'use', 'would', 'im', 'go', 'ive', 'puregym', 'pure', 'really', 'need'}\n",
        "\n",
        "# Update stopwords list to include custom stopwords\n",
        "stop_words = stop_words.union(custom_stopwords)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers and punctuation\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and single characters\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3xmEnmjeuoT"
      },
      "outputs": [],
      "source": [
        "# Apply to Google and Trustpilot reviews\n",
        "google_df['clean_tokens'] = google_df['Comment'].apply(preprocess_text)\n",
        "trustpilot_df['clean_tokens'] = trustpilot_df['Review Content'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqefWAPmogtE"
      },
      "outputs": [],
      "source": [
        "# Flatten the list of tokens for each dataset\n",
        "google_words = [word for tokens in google_df['clean_tokens'] for word in tokens]\n",
        "trustpilot_words = [word for tokens in trustpilot_df['clean_tokens'] for word in tokens]\n",
        "\n",
        "# Frequency distribution\n",
        "google_freq = FreqDist(google_words)\n",
        "trustpilot_freq = FreqDist(trustpilot_words)\n",
        "\n",
        "# Top 10 words\n",
        "google_top10 = google_freq.most_common(10)\n",
        "trustpilot_top10 = trustpilot_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA5dDIYUosJE"
      },
      "outputs": [],
      "source": [
        "# Google\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=[w[0] for w in google_top10], y=[w[1] for w in google_top10], palette='Blues_d')\n",
        "plt.title('Top 10 Words in Google Reviews')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Trustpilot\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=[w[0] for w in trustpilot_top10], y=[w[1] for w in trustpilot_top10], palette='Greens_d')\n",
        "plt.title('Top 10 Words in Trustpilot Reviews')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lfMrtqKpT_r"
      },
      "outputs": [],
      "source": [
        "# Join all tokens for wordcloud\n",
        "google_text = ' '.join(google_words)\n",
        "trustpilot_text = ' '.join(trustpilot_words)\n",
        "\n",
        "# Google WordCloud\n",
        "plt.figure(figsize=(8,6))\n",
        "wc = WordCloud(width=800, height=400, background_color='white').generate(google_text)\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Google Reviews Word Cloud')\n",
        "plt.show()\n",
        "\n",
        "# Trustpilot WordCloud\n",
        "plt.figure(figsize=(8,6))\n",
        "wc = WordCloud(width=800, height=400, background_color='white').generate(trustpilot_text)\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Trustpilot Reviews Word Cloud')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Kl5lJjmpj5t"
      },
      "outputs": [],
      "source": [
        "google_df.to_csv('Google_12_months_cleaned.csv', index=False)\n",
        "trustpilot_df.to_csv('Trustpilot_12_months_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAHoGdz6rufv"
      },
      "outputs": [],
      "source": [
        "# For Google reviews (negative if Overall Score < 3)\n",
        "google_neg = google_df[google_df['Overall Score'] < 3].copy()\n",
        "\n",
        "# For Trustpilot reviews (negative if Review Stars < 3)\n",
        "trustpilot_neg = trustpilot_df[trustpilot_df['Review Stars'] < 3].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRF9nL_8sOTR"
      },
      "outputs": [],
      "source": [
        "# Calculate number and percentage of negative reviews\n",
        "num_google_neg = len(google_neg)\n",
        "num_trustpilot_neg = len(trustpilot_neg)\n",
        "percent_google_neg = (num_google_neg / len(google_df)) * 100\n",
        "percent_trustpilot_neg = (num_trustpilot_neg / len(trustpilot_df)) * 100\n",
        "\n",
        "print(f\"Number of negative Google reviews: {num_google_neg} ({percent_google_neg:.2f}%)\")\n",
        "print(f\"Number of negative Trustpilot reviews: {num_trustpilot_neg} ({percent_trustpilot_neg:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yJqs2QNsqld"
      },
      "outputs": [],
      "source": [
        "# Flatten the token list for negative reviews\n",
        "google_neg_words = [word for tokens in google_neg['clean_tokens'] for word in tokens]\n",
        "trustpilot_neg_words = [word for tokens in trustpilot_neg['clean_tokens'] for word in tokens]\n",
        "\n",
        "# Frequency distribution\n",
        "google_neg_freq = FreqDist(google_neg_words)\n",
        "trustpilot_neg_freq = FreqDist(trustpilot_neg_words)\n",
        "\n",
        "# Top 10 words\n",
        "google_neg_top10 = google_neg_freq.most_common(10)\n",
        "trustpilot_neg_top10 = trustpilot_neg_freq.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aIJ8ph5svX6"
      },
      "outputs": [],
      "source": [
        "# Google negative reviews\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=[w[0] for w in google_neg_top10], y=[w[1] for w in google_neg_top10], palette='Reds_d')\n",
        "plt.title('Top 10 Words in Negative Google Reviews')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Trustpilot negative reviews\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=[w[0] for w in trustpilot_neg_top10], y=[w[1] for w in trustpilot_neg_top10], palette='Oranges_d')\n",
        "plt.title('Top 10 Words in Negative Trustpilot Reviews')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HkYxW-etBCu"
      },
      "outputs": [],
      "source": [
        "# Join all tokens for wordcloud\n",
        "google_neg_text = ' '.join(google_neg_words)\n",
        "trustpilot_neg_text = ' '.join(trustpilot_neg_words)\n",
        "\n",
        "# Google Negative WordCloud\n",
        "plt.figure(figsize=(8,6))\n",
        "wc = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(google_neg_text)\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Negative Google Reviews Word Cloud')\n",
        "plt.show()\n",
        "\n",
        "# Trustpilot Negative WordCloud\n",
        "plt.figure(figsize=(8,6))\n",
        "wc = WordCloud(width=800, height=400, background_color='white', colormap='Oranges').generate(trustpilot_neg_text)\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Negative Trustpilot Reviews Word Cloud')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwVK-_EFvKtz"
      },
      "outputs": [],
      "source": [
        "# Filter Negative Reviews for Common Locations\n",
        "\n",
        "# Identify location columns\n",
        "google_location_col = \"Club's Name\"\n",
        "trustpilot_location_col = \"Location Name\"\n",
        "\n",
        "# Standardize location columns for both negative review dataframes\n",
        "google_neg['location_std'] = google_neg[google_location_col].str.strip().str.lower()\n",
        "trustpilot_neg['location_std'] = trustpilot_neg[trustpilot_location_col].str.strip().str.lower()\n",
        "\n",
        "# Find common locations using standardized names\n",
        "common_locations = set(google_neg['location_std'].unique()) & set(trustpilot_neg['location_std'].unique())\n",
        "print(f\"Number of common locations: {len(common_locations)}\")\n",
        "\n",
        "# Filter negative reviews to only those from common locations (using standardized column)\n",
        "google_neg_common = google_neg[google_neg['location_std'].isin(common_locations)]\n",
        "trustpilot_neg_common = trustpilot_neg[trustpilot_neg['location_std'].isin(common_locations)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_neg_common['Source'] = 'Google'\n",
        "trustpilot_neg_common['Source'] = 'Trustpilot'\n",
        "\n",
        "# Unify the review columns\n",
        "google_neg_common = google_neg_common.rename(columns={'Comment': 'Review'})\n",
        "trustpilot_neg_common = trustpilot_neg_common.rename(columns={'Review Content': 'Review'})\n",
        "\n",
        "merged_neg_reviews = pd.concat(\n",
        "    [google_neg_common[['location_std', 'Review', 'Source']],\n",
        "     trustpilot_neg_common[['location_std', 'Review', 'Source']]],\n",
        "     ignore_index=True\n",
        ")"
      ],
      "metadata": {
        "id": "kq1m9RaXv184"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Merged Data for Later Use\n",
        "merged_neg_reviews.to_csv('merged_neg_reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "g-eH3V0K2MKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Merged Data for Later Use\n",
        "merged_neg_reviews.to_csv('merged_neg_reviews.csv', index=False)\n",
        "Preprocess this data set for BERTopic.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return ' '.join(tokens)  # Re-join tokens into a string\n",
        "\n",
        "# Apply to my merged DataFrame\n",
        "merged_neg_reviews['cleaned_review'] = merged_neg_reviews['Review'].astype(str).apply(preprocess_text)"
      ],
      "metadata": {
        "id": "IszWXtQm4jGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of cleaned reviews, ready for BERTopic modeling\n",
        "all_neg_reviews_common_cleaned = merged_neg_reviews['cleaned_review'].tolist()"
      ],
      "metadata": {
        "id": "O0K4y7Bd50v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first few entries in the cleaned list\n",
        "print(all_neg_reviews_common_cleaned[:10])  # Printing the first 10 elements to inspect\n"
      ],
      "metadata": {
        "id": "NpeJPYyk8XTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhGh16dMv1e9"
      },
      "outputs": [],
      "source": [
        "# Create and fit BERTopic model## Remove empty or whitespace-only reviews\n",
        "all_neg_reviews_common_cleaned = [review for review in all_neg_reviews_common_cleaned if review.strip() != '']\n",
        "topic_model = BERTopic(language=\"english\", verbose=True)\n",
        "topics, probs = topic_model.fit_transform(all_neg_reviews_common_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic information as a DataFrame\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Exclude the outlier topic (-1), which is usually \"no topic\"\n",
        "topic_info = topic_info[topic_info.Topic != -1]\n",
        "\n",
        "# Display the top N topics (by frequency)\n",
        "N = 10  # or however many you want to display\n",
        "print(\"Top topics and their document frequencies:\")\n",
        "print(topic_info[['Topic', 'Count', 'Name']].head(N))"
      ],
      "metadata": {
        "id": "2NtlAkeZ7zHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 2 topics by frequency (excluding -1, which is usually \"outlier\" or \"no topic\")\n",
        "top_topics = topic_info[topic_info.Topic != -1].head(2)\n",
        "\n",
        "for idx, row in top_topics.iterrows():\n",
        "    topic_num = row['Topic']\n",
        "    top_words = [word for word, _ in topic_model.get_topic(topic_num)]\n",
        "    print(f\"Top words for topic {topic_num}: {', '.join(top_words[:10])}\")"
      ],
      "metadata": {
        "id": "tg29hdCA62GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvUxhOMyNeAT"
      },
      "outputs": [],
      "source": [
        "fig = topic_model.visualize_topics()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1sDFbZzG7Q7"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_barchart(top_n_topics=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmzO9fZbG_YC"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WVIVFi4HZi8"
      },
      "outputs": [],
      "source": [
        "# Show top 10 topics and their most representative words\n",
        "for topic_id in topic_info['Topic'].head(10):\n",
        "    if topic_id == -1:  # -1 is often the outlier/noise topic\n",
        "        continue\n",
        "    print(f\"\\nTopic {topic_id}:\")\n",
        "    words = topic_model.get_topic(topic_id)  # List of (word, weight)\n",
        "    print(\"Top words:\", ', '.join([w[0] for w in words[:8]]))\n",
        "    # Optionally, print a few example documents from each topi_c:\n",
        "    docs = [all_neg_reviews_common_cleaned[i] for i, t in enumerate(topics) if t == topic_id][:2]\n",
        "    print(\"Example reviews:\", docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEX00limK_ek"
      },
      "source": [
        "# **4. Location Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhr2dr6nLTc1"
      },
      "outputs": [],
      "source": [
        "# Location Analysis\n",
        "# Find Top 20 Locations with the Most Negative Reviews in Each Dataset\n",
        "# List out the top 20 locations with the highest number of negative reviews. Do this separately for Google and Trustpilot's reviews, and comment on the result.\n",
        "\n",
        "# For Google\n",
        "google_neg_location_counts = google_neg.groupby(\"Club's Name\").size().sort_values(ascending=False)\n",
        "google_top20_locs = google_neg_location_counts.head(20)\n",
        "print(\"Top 20 Google locations with most negative reviews:\\n\", google_top20_locs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOehY32SLYJL"
      },
      "outputs": [],
      "source": [
        "# For Trustpilot\n",
        "trustpilot_neg_location_counts = trustpilot_neg.groupby(\"Location Name\").size().sort_values(ascending=False)\n",
        "trustpilot_top20_locs = trustpilot_neg_location_counts.head(20)\n",
        "print(\"Top 20 Trustpilot locations with most negative reviews:\\n\", trustpilot_top20_locs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the set of top 20 locations from each dataset\n",
        "google_top20_set = set(google_top20_locs.index)\n",
        "trustpilot_top20_set = set(trustpilot_top20_locs.index)\n",
        "\n",
        "# Find the intersection (common locations)\n",
        "common_locations = google_top20_set.intersection(trustpilot_top20_set)\n",
        "\n",
        "# Calculate the number and percentage of common locations\n",
        "num_common = len(common_locations)\n",
        "total_possible = min(len(google_top20_set), len(trustpilot_top20_set))  # usually 20\n",
        "percentage_common = (num_common / total_possible) * 100\n",
        "\n",
        "# Display results\n",
        "print(f\"Common locations in both data sets ({num_common} out of {total_possible}, {percentage_common:.1f}%):\")\n",
        "for loc in common_locations:\n",
        "    print(f\"- {loc}\")"
      ],
      "metadata": {
        "id": "dEzmLstzFPjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the 2 data sets using Location Name and Club's Name.\n",
        "\n",
        "# Now, list out the following:\n",
        "# • Locations\n",
        "# • Number of Trustpilot reviews for this location\n",
        "# • Number of Google reviews for this location\n",
        "# • Total number of reviews for this location (sum of Google reviews and Trustpilot reviews)\n",
        "# Sort based on the total number of reviews.\n",
        "\n",
        "# Extract top 20 locations from each dataset\n",
        "google_top20_locs = google_neg.groupby(\"Club's Name\").size().sort_values(ascending=False).head(20)\n",
        "trustpilot_top20_locs = trustpilot_neg.groupby(\"Location Name\").size().sort_values(ascending=False).head(20)\n",
        "\n",
        "# Step 1: Create DataFrames for both Google and Trustpilot review counts\n",
        "google_reviews = google_top20_locs.reset_index(name=\"Google Reviews\")\n",
        "google_reviews = google_reviews.rename(columns={\"Club's Name\": \"Location Name\"})  # Ensure consistent column name\n",
        "\n",
        "trustpilot_reviews = trustpilot_top20_locs.reset_index(name=\"Trustpilot Reviews\")\n",
        "trustpilot_reviews = trustpilot_reviews.rename(columns={\"Location Name\": \"Location Name\"})\n",
        "\n",
        "# Step 2: Merge the two DataFrames on 'Location Name', using outer join to include all unique locations\n",
        "merged_reviews = pd.merge(google_reviews, trustpilot_reviews, on=\"Location Name\", how=\"outer\")\n",
        "\n",
        "# Step 3: Fill NaN values with 0 (in case a location doesn't have reviews in either dataset)\n",
        "merged_reviews = merged_reviews.fillna(0)\n",
        "\n",
        "# Step 4: Calculate the total reviews (sum of Google and Trustpilot reviews)\n",
        "merged_reviews[\"Total Reviews\"] = merged_reviews[\"Google Reviews\"] + merged_reviews[\"Trustpilot Reviews\"]\n",
        "\n",
        "# Step 5: Sort by total reviews (descending)\n",
        "merged_reviews_sorted = merged_reviews.sort_values(by=\"Total Reviews\", ascending=False)\n",
        "\n",
        "# Step 6: Display the table in a nice format using tabulate\n",
        "formatted_table = tabulate(\n",
        "    merged_reviews_sorted[['Location Name', 'Trustpilot Reviews', 'Google Reviews', 'Total Reviews']].values,\n",
        "    headers=['Location Name', 'Trustpilot Reviews', 'Google Reviews', 'Total Reviews'],\n",
        "    tablefmt='grid'\n",
        ")\n",
        "\n",
        "# Print the formatted table\n",
        "print(formatted_table)"
      ],
      "metadata": {
        "id": "jT_v68hOIlbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged and sorted DF\n",
        "merged_reviews_sorted.to_csv(\"top_merged_location_reviews.csv\", index=False)"
      ],
      "metadata": {
        "id": "SzysxfPYSnqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 30 locations by total reviews\n",
        "top30_locations = merged_reviews_sorted.head(30)\n",
        "\n",
        "# Print just the locations\n",
        "print(\"Top 30 Locations Based on Total Reviews:\\n\")\n",
        "for location in top30_locations['Location Name']:\n",
        "    print(location)"
      ],
      "metadata": {
        "id": "Zo_ROEZ7VMFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top30_locations.columns)"
      ],
      "metadata": {
        "id": "OnaXi_KOrTjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the negative reviews from both Google and Trustpilot datasets for top 30 locations\n",
        "\n",
        "def normalize_location(name):\n",
        "    if pd.isna(name):\n",
        "        return \"\"\n",
        "    # Convert to string first, then strip and lowercase\n",
        "    return str(name).strip().lower()\n",
        "\n",
        "# dataframe column:\n",
        "top30_locations['location_std'] = top30_locations['Location Name'].apply(normalize_location)\n",
        "\n",
        "# Normalize location names in google_neg and trustpilot_neg\n",
        "google_neg['location_std'] = google_neg[\"Club's Name\"].apply(normalize_location)\n",
        "trustpilot_neg['location_std'] = trustpilot_neg[\"Location Name\"].apply(normalize_location)\n",
        "\n",
        "# Get list of normalized top 30 locations\n",
        "top30_locs_std = top30_locations['location_std'].unique().tolist()\n",
        "\n",
        "# Filter negative reviews for top 30 locations using normalized column\n",
        "google_neg_top30 = google_neg[google_neg['location_std'].isin(top30_locs_std)].copy()\n",
        "trustpilot_neg_top30 = trustpilot_neg[trustpilot_neg['location_std'].isin(top30_locs_std)].copy()\n",
        "\n",
        "# Rename columns for consistency\n",
        "google_neg_top30 = google_neg_top30.rename(columns={\"Club's Name\": \"Location\", \"Comment\": \"Review\"})\n",
        "trustpilot_neg_top30 = trustpilot_neg_top30.rename(columns={\"Location Name\": \"Location\", \"Review Content\": \"Review\"})\n",
        "\n",
        "# Add source columns\n",
        "google_neg_top30['Source'] = 'Google'\n",
        "trustpilot_neg_top30['Source'] = 'Trustpilot'\n",
        "\n",
        "# Select only needed columns\n",
        "google_subset = google_neg_top30[['Location', 'Review', 'Source']]\n",
        "trustpilot_subset = trustpilot_neg_top30[['Location', 'Review', 'Source']]\n",
        "\n",
        "# Combine datasets\n",
        "combined_neg_reviews = pd.concat([google_subset, trustpilot_subset], ignore_index=True)\n",
        "\n",
        "print(f\"Combined negative reviews count: {combined_neg_reviews.shape[0]}\")\n",
        "print(combined_neg_reviews.head())"
      ],
      "metadata": {
        "id": "EvfBa7MSrsgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_neg_reviews['Source'].value_counts())"
      ],
      "metadata": {
        "id": "vHyzQmhCsPrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the top 30 locations, redo the word frequency and word cloud. Comment on the results, and highlight\n",
        "# if the results are different from the first run.\n",
        "\n",
        "# Combine all reviews into a single string (lowercase, remove punctuation)\n",
        "all_reviews = \" \".join(combined_neg_reviews['Review'].dropna().astype(str)).lower()\n",
        "\n",
        "# Remove punctuation and digits\n",
        "all_reviews_clean = re.sub(r'[^a-z\\s]', '', all_reviews)\n",
        "\n",
        "# Define the stopwords (from NLTK)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Custom stopwords you want to remove manually\n",
        "custom_stopwords = {'get', 'one', 'always', 'also', 'dont', 'even', 'like', 'use', 'would', 'im', 'go', 'ive', 'puregym', 'pure'}\n",
        "\n",
        "# Update stopwords list to include custom stopwords\n",
        "combined_stopwords = stop_words.union(custom_stopwords)\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "tokens = [word for word in all_reviews_clean.split() if word not in combined_stopwords]\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(tokens)\n",
        "\n",
        "# Display top 30 words\n",
        "top_10_words = word_freq.most_common(10)\n",
        "print(\"Top 10 most common words in negative reviews (top 30 locations):\")\n",
        "for word, freq in top_10_words:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "id": "zc8BsHu5x4N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack words and frequencies from top_30_words\n",
        "words = [w[0] for w in top_10_words]\n",
        "freqs = [w[1] for w in top_10_words]\n",
        "\n",
        "# Plot barplot (histogram-like)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=words, y=freqs, palette='Purples_d')\n",
        "plt.title('Top 10 Words in Negative Reviews (Top 30 Locations)')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KsXhdvLiwQr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUzi0zY0xj6D"
      },
      "outputs": [],
      "source": [
        "# Create a WordCloud from the top 10 words (we only use words and frequencies)\n",
        "top_10_dict = dict(top_10_words)\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_10_dict)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis\n",
        "plt.title('Top 10 Words in Negative Reviews')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Redo Topic Modelling with BERTopic for top 30\n",
        "\n",
        "# Clean each review individually\n",
        "cleaned_reviews = []\n",
        "\n",
        "# Define the stopwords (from NLTK)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Custom stopwords you want to remove manually\n",
        "custom_stopwords = {'get', 'one', 'always', 'also', 'dont', 'even', 'like', 'use', 'would', 'im', 'go', 'ive', 'puregym', 'pure'}\n",
        "\n",
        "# Update stopwords list to include custom stopwords\n",
        "combined_stopwords = stop_words.union(custom_stopwords)\n",
        "\n",
        "# Iterate over each review and clean it\n",
        "for review in combined_neg_reviews['Review'].dropna():\n",
        "    # Convert to lowercase and remove non-alphabetic characters\n",
        "    review_clean = re.sub(r'[^a-z\\s]', '', review.lower())\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = [word for word in review_clean.split() if word not in combined_stopwords]\n",
        "\n",
        "    # Join tokens back into a single cleaned review string\n",
        "    cleaned_reviews.append(' '.join(tokens))\n",
        "\n",
        "# Initialize the BERTopic model\n",
        "topic_model_neg_reviews = BERTopic(language=\"english\", verbose=True)\n",
        "\n",
        "# Fit the model to the cleaned reviews and get topics and probabilities\n",
        "topics_neg_reviews, probs_neg_reviews = topic_model_neg_reviews.fit_transform(cleaned_reviews)"
      ],
      "metadata": {
        "id": "RksjSjuO5IjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic information as a DataFrame\n",
        "topic_info = topic_model_neg_reviews.get_topic_info()\n",
        "\n",
        "# Exclude the outlier topic (-1), which is usually \"no topic\"\n",
        "topic_info = topic_info[topic_info.Topic != -1]\n",
        "\n",
        "# Display the top N topics (by frequency)\n",
        "N = 10  # or however many you want to display\n",
        "print(\"Top topics and their document frequencies:\")\n",
        "print(topic_info[['Topic', 'Count', 'Name']].head(N))"
      ],
      "metadata": {
        "id": "fjmTSjuy8gPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHheDldCzj3T"
      },
      "outputs": [],
      "source": [
        "# Bar Chart: Top Words per Topic for top 30\n",
        "topic_model_neg_reviews.visualize_barchart(top_n_topics=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdBKzZr7zcqH"
      },
      "outputs": [],
      "source": [
        "# Heatmap: Topic Similarities for top 30\n",
        "topic_model_neg_reviews.visualize_topics()\n",
        "topic_model_neg_reviews.visualize_barchart(top_n_topics=10)\n",
        "topic_model_neg_reviews.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhovUKsp9UZl"
      },
      "outputs": [],
      "source": [
        "# ----------Emotion Analysis--------------\n",
        "# Note: nateraw/bert-base-uncased-emotion model predicts emotions anger, disgust, fear, joy, sadness and surprise (six in total).\n",
        "# Use BERT Emotion Classifier to Analyze Negative Reviews\n",
        "# Import the BERT model bhadresh-savani/bert-base-uncased-emotion from Hugging Face, and set up a pipeline for text classification.\n",
        "\n",
        "# Set up the Hugging Face pipeline for emotion classification\n",
        "emotion_classifier = pipeline(\"text-classification\", model=\"nateraw/bert-base-uncased-emotion\", tokenizer=\"nateraw/bert-base-uncased-emotion\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence\n",
        "text = \"Every time I see my dog, my heart fills with love and happiness.\"\n",
        "\n",
        "# Get emotion prediction for the sentence\n",
        "emotion_result = emotion_classifier(text)\n",
        "\n",
        "# Print the classification result\n",
        "print(emotion_result)"
      ],
      "metadata": {
        "id": "en1oEzMgJNkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this model on both data sets\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set up the Hugging Face pipeline for emotion classification\n",
        "emotion_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"nateraw/bert-base-uncased-emotion\",\n",
        "    tokenizer=\"nateraw/bert-base-uncased-emotion\",\n",
        "    device=0  # Set to -1 for CPU, 0 for GPU\n",
        ")\n",
        "\n",
        "# Modify the function to handle truncation and padding explicitly\n",
        "def get_top_emotion(review):\n",
        "    # Tokenize with truncation and padding\n",
        "    result = emotion_classifier(review, padding=True, truncation=True, max_length=512)\n",
        "    # Return the predicted label (emotion)\n",
        "    return result[0]['label']\n",
        "\n",
        "# Sampling a smaller portion of the data\n",
        "sample_size = 1000  # To reduce runtime; can be changed\n",
        "\n",
        "# Sample 100 random rows from the Google reviews dataframe\n",
        "google_sample = google_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Sample 100 random rows from the Trustpilot reviews dataframe\n",
        "trustpilot_sample = trustpilot_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Apply the emotion prediction to the sampled Google and Trustpilot reviews\n",
        "google_sample['predicted_emotion'] = google_sample['Comment'].apply(get_top_emotion)\n",
        "trustpilot_sample['predicted_emotion'] = trustpilot_sample['Review Content'].apply(get_top_emotion)\n",
        "\n",
        "# Print the results for the sample\n",
        "print(\"Sampled Google Reviews Emotions:\")\n",
        "print(google_sample[['Comment', 'predicted_emotion']])\n",
        "\n",
        "print(\"\\nSampled Trustpilot Reviews Emotions:\")\n",
        "print(trustpilot_sample[['Review Content', 'predicted_emotion']])"
      ],
      "metadata": {
        "id": "auXD3rpETkwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture the top emotion for each review\n",
        "\n",
        "# Find the top 3 most common emotions in the sampled reviews\n",
        "google_top_3_emotions = google_sample['predicted_emotion'].value_counts().head(3)\n",
        "trustpilot_top_3_emotions = trustpilot_sample['predicted_emotion'].value_counts().head(3)\n",
        "\n",
        "print(\"\\nTop 3 Emotions in Google Reviews Sample:\")\n",
        "print(google_top_3_emotions)\n",
        "\n",
        "print(\"\\nTop 3 Emotions in Trustpilot Reviews Sample:\")\n",
        "print(trustpilot_top_3_emotions)"
      ],
      "metadata": {
        "id": "z-0vXdGfVewG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a bar plot to show the top emotion distribution for all negative reviews in both data sets.\n",
        "\n",
        "# Define a list of negative emotions\n",
        "negative_emotions = ['sadness', 'anger', 'fear']\n",
        "\n",
        "# Filter the Google reviews to keep only negative emotions\n",
        "google_negative = google_sample[google_sample['predicted_emotion'].isin(negative_emotions)]\n",
        "\n",
        "# Filter the Trustpilot reviews to keep only negative emotions\n",
        "trustpilot_negative = trustpilot_sample[trustpilot_sample['predicted_emotion'].isin(negative_emotions)]\n",
        "\n",
        "# Combine the data for both datasets\n",
        "combined_negative_emotions = pd.concat([google_negative['predicted_emotion'], trustpilot_negative['predicted_emotion']], axis=0)\n",
        "\n",
        "# Plot the distribution of negative emotions\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=combined_negative_emotions, palette='coolwarm')\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title('Distribution of Negative Emotions in Google and Trustpilot Reviews', fontsize=14)\n",
        "plt.xlabel('Negative Emotions', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UhT8z-awYyE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is to check if the bar chart makes sense\n",
        "\n",
        "# Count the occurrences of each emotion in the sampled data for Google and Trustpilot\n",
        "google_emotion_counts = google_sample['predicted_emotion'].value_counts()\n",
        "trustpilot_emotion_counts = trustpilot_sample['predicted_emotion'].value_counts()\n",
        "\n",
        "# Print the distribution of emotions in the samples\n",
        "print(\"Emotion distribution in Google Reviews Sample:\")\n",
        "print(google_emotion_counts)\n",
        "\n",
        "print(\"\\nEmotion distribution in Trustpilot Reviews Sample:\")\n",
        "print(trustpilot_emotion_counts)\n"
      ],
      "metadata": {
        "id": "OSePfb9kZ7cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all the negative reviews (from both data sets) where anger is top emotion.\n",
        "\n",
        "# Extract reviews from Google where 'Anger' is the predicted emotion\n",
        "google_anger_reviews = google_sample[google_sample['predicted_emotion'] == 'anger']\n",
        "\n",
        "# Extract reviews from Trustpilot where 'Anger' is the predicted emotion\n",
        "trustpilot_anger_reviews = trustpilot_sample[trustpilot_sample['predicted_emotion'] == 'anger']\n",
        "\n",
        "# Print the reviews with 'Anger' emotion for both datasets\n",
        "print(\"Google Reviews with 'Anger' Emotion:\")\n",
        "print(google_anger_reviews[['Comment', 'predicted_emotion']])\n",
        "\n",
        "print(\"\\nTrustpilot Reviews with 'Anger' Emotion:\")\n",
        "print(trustpilot_anger_reviews[['Review Content', 'predicted_emotion']])"
      ],
      "metadata": {
        "id": "1q_HPSrKZtCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run BERTopic on the output of the previous step.\n",
        "\n",
        "def preprocess_text(text):\n",
        " text = text.lower()\n",
        " text = re.sub(r'\\d+', '', text)\n",
        " text = text.translate(str.maketrans('', '', string.punctuation))\n",
        " tokens = word_tokenize(text)\n",
        " tokens = [word for word in tokens if word not in stop_words and len(word) >1]\n",
        " return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to the anger reviews\n",
        "google_anger_reviews['cleaned_review'] = google_anger_reviews['Comment'].apply(preprocess_text)\n",
        "trustpilot_anger_reviews['cleaned_review'] = trustpilot_anger_reviews['Review Content'].apply(preprocess_text)\n",
        "\n",
        "# Combine both datasets for topic modeling\n",
        "all_anger_reviews = pd.concat([google_anger_reviews['cleaned_review'], trustpilot_anger_reviews['cleaned_review']], axis=0).tolist()\n",
        "\n",
        "# Remove any empty strings from the list\n",
        "all_anger_reviews = [review for review in all_anger_reviews if review.strip() != '']\n",
        "\n",
        "# Set up the BERTopic model with a minimum number of topics\n",
        "topic_model = BERTopic(language=\"english\", verbose=True, min_topic_size=5)\n",
        "\n",
        "# Fit the model to the anger reviews\n",
        "topics, probs = topic_model.fit_transform(all_anger_reviews)\n",
        "\n",
        "print(topic_info[['Topic', 'Count', 'Name']].head(10))"
      ],
      "metadata": {
        "id": "qN90QMLtbCCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the top 5 topics as a bar chart\n",
        "topic_model.visualize_barchart(top_n_topics=5)"
      ],
      "metadata": {
        "id": "AvH_7_9-kzcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_neg_reviews_common_cleaned))"
      ],
      "metadata": {
        "id": "IdFw_H8Y_AG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topics[:20])  # See the first 20 topic assignments\n",
        "print(set(topics))  # See all unique topics assigned"
      ],
      "metadata": {
        "id": "YrSbywr8_Ko0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topics with a heatmap\n",
        "topic_model.visualize_topics()\n",
        "topic_model.visualize_barchart(top_n_topics=10)\n",
        "topic_model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "638wkGebmGHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}