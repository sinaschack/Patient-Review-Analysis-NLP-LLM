{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2R4SZQVJJIG"
      },
      "outputs": [],
      "source": [
        "# Change to GPU for parallel processing.\n",
        "# Here, for the below code to return true, I choose Runtime → Change runtime type → T4 GPU\n",
        "\n",
        "import torch\n",
        "print(torch.cuda.is_available())  # Should be true\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install -U transformers accelerate bitsandbytes huggingface_hub\n",
        "!pip install umap-learn\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install gensim pyldavis\n",
        "!pip uninstall -y openai httpx\n",
        "!pip install httpx==0.24.1\n",
        "!pip install openai==0.28\n",
        "!pip install bertopic[hdbscan]\n",
        "!pip uninstall -y openai\n",
        "!pip install bertopic[hdbscan] sentence-transformers\n",
        "!pip install bertopic[hdbscan]"
      ],
      "metadata": {
        "id": "2t723-epSICp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9R_15-XHWZJ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0o-1uhmHWZJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZOwK-9eHWZK"
      },
      "outputs": [],
      "source": [
        "# Load data from source\n",
        "\n",
        "# Import the drive module from Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive so Colab can access your files\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbKD0MiwHWZK"
      },
      "outputs": [],
      "source": [
        "# Define the full paths to the excel files\n",
        "google_path = '/content/drive/My Drive/Colab Notebooks/google_reviews_synthetic.csv'\n",
        "trustpilot_path = '/content/drive/My Drive/Colab Notebooks/trustpilot_reviews_synthetic.csv'\n",
        "\n",
        "# Load the reviews CSV files into DataFrames\n",
        "google_df = pd.read_csv(google_path)\n",
        "trustpilot_df = pd.read_csv(trustpilot_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVJ10EwHHWZK"
      },
      "outputs": [],
      "source": [
        "print(google_df.columns)\n",
        "print(trustpilot_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzTEtrtTHWZM"
      },
      "outputs": [],
      "source": [
        "# Data cleaning\n",
        "# Remove rows with missing review text\n",
        "google_df = google_df.dropna(subset=['Comment'])\n",
        "trustpilot_df = trustpilot_df.dropna(subset=['Review Content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPYVjl8qHWZM"
      },
      "outputs": [],
      "source": [
        "# ----------------Preprocessing: lowercase, remove stopwords, remove numbers-------------\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers and punctuation\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and single characters\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPShluMQHWZN"
      },
      "outputs": [],
      "source": [
        "# Apply to Google and Trustpilot reviews\n",
        "google_df['clean_tokens'] = google_df['Comment'].apply(preprocess_text)\n",
        "trustpilot_df['clean_tokens'] = trustpilot_df['Review Content'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0vFyRtEKST3"
      },
      "outputs": [],
      "source": [
        "# Identify Negative Reviews\n",
        "\n",
        "# For Google reviews (negative if Overall Score < 3)\n",
        "google_neg = google_df[google_df['Overall Score'] < 3].copy()\n",
        "\n",
        "# For Trustpilot reviews (negative if Review Stars < 3)\n",
        "trustpilot_neg = trustpilot_df[trustpilot_df['Review Stars'] < 3].copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe results using pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Define paths to save the pickle files in your Google Drive\n",
        "google_neg_path = '/content/drive/My Drive/Colab Notebooks/google_neg_reviews.pkl'\n",
        "trustpilot_neg_path = '/content/drive/My Drive/Colab Notebooks/trustpilot_neg_reviews.pkl'\n",
        "\n",
        "# Save Google negative reviews\n",
        "with open(google_neg_path, 'wb') as f:\n",
        "    pickle.dump(google_neg, f)\n",
        "\n",
        "# Save Trustpilot negative reviews\n",
        "with open(trustpilot_neg_path, 'wb') as f:\n",
        "    pickle.dump(trustpilot_neg, f)"
      ],
      "metadata": {
        "id": "NoNk4kgm1pFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define paths to save the pickle files in your Google Drive\n",
        "google_neg_path = '/content/drive/My Drive/Colab Notebooks/google_neg_reviews.pkl'\n",
        "trustpilot_neg_path = '/content/drive/My Drive/Colab Notebooks/trustpilot_neg_reviews.pkl'\n",
        "\n",
        "# Load (unpickle) the DataFrames\n",
        "with open(google_neg_path, 'rb') as f:\n",
        "    google_neg = pickle.load(f)\n",
        "\n",
        "with open(trustpilot_neg_path, 'rb') as f:\n",
        "    trustpilot_neg = pickle.load(f)"
      ],
      "metadata": {
        "id": "NaUqZAY14F4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcsDc6nCwMgL"
      },
      "outputs": [],
      "source": [
        "# Load the following model: tiiuae/falcon-7b-instruct. Set the pipeline for text generation and a max length of 1,000 for each review.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# Configuration for 8-bit quantization (for better memory usage)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # Offload some parts to CPU automatically\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model with the specified configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Automatically choose the best device (GPU/CPU)\n",
        "    trust_remote_code=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "-ZUx1dplJfIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Main Topics from Each Negative Review (Prompted with Falcon-7b-Instruct)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from langdetect import detect, LangDetectException\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# Topic normalization dictionary\n",
        "# -------------------------------\n",
        "topic_normalization = {\n",
        "    # Cleanliness & Hygiene\n",
        "    \"dirty\": \"Hospital Cleanliness\",\n",
        "    \"hygiene\": \"Hospital Cleanliness\",\n",
        "    \"sanitary\": \"Hospital Cleanliness\",\n",
        "    \"filthy\": \"Hospital Cleanliness\",\n",
        "    \"infection\": \"Hospital Cleanliness\",\n",
        "    \"sterile\": \"Hospital Cleanliness\",\n",
        "    \"disinfection\": \"Hospital Cleanliness\",\n",
        "\n",
        "    # Parking & Access\n",
        "    \"parking\": \"Parking Issues\",\n",
        "    \"car park\": \"Parking Issues\",\n",
        "    \"space\": \"Parking Issues\",\n",
        "    \"access\": \"Access Issues\",\n",
        "    \"wheelchair\": \"Accessibility\",\n",
        "    \"ramp\": \"Accessibility\",\n",
        "\n",
        "    # Pricing & Billing\n",
        "    \"price\": \"Pricing\",\n",
        "    \"charges\": \"Pricing\",\n",
        "    \"expensive\": \"Pricing\",\n",
        "    \"cost\": \"Pricing\",\n",
        "    \"billing\": \"Billing Issues\",\n",
        "    \"insurance\": \"Billing Issues\",\n",
        "\n",
        "    # Staff Behavior & Communication\n",
        "    \"nurse\": \"Nurse Behavior\",\n",
        "    \"doctor\": \"Doctor Behavior\",\n",
        "    \"rude\": \"Staff Rudeness\",\n",
        "    \"friendly\": \"Staff Friendliness\",\n",
        "    \"unprofessional\": \"Staff Rudeness\",\n",
        "    \"communication\": \"Communication Quality\",\n",
        "    \"explained\": \"Communication Quality\",\n",
        "    \"updates\": \"Communication Quality\",\n",
        "\n",
        "    # Waiting Times\n",
        "    \"waiting\": \"Waiting Times\",\n",
        "    \"delay\": \"Waiting Times\",\n",
        "    \"queue\": \"Waiting Times\",\n",
        "    \"appointment\": \"Waiting Times\",\n",
        "\n",
        "    # Treatment & Care Quality\n",
        "    \"treatment\": \"Treatment Quality\",\n",
        "    \"procedure\": \"Treatment Quality\",\n",
        "    \"operation\": \"Treatment Quality\",\n",
        "    \"surgery\": \"Treatment Quality\",\n",
        "    \"specialist\": \"Doctor Specialty\",\n",
        "    \"expert\": \"Doctor Specialty\",\n",
        "    \"cardiology\": \"Doctor Specialty\",\n",
        "    \"orthopedic\": \"Doctor Specialty\",\n",
        "\n",
        "    # Facilities & Equipment\n",
        "    \"equipment\": \"Medical Equipment\",\n",
        "    \"machines\": \"Medical Equipment\",\n",
        "    \"facility\": \"Hospital Facilities\",\n",
        "    \"room\": \"Hospital Facilities\",\n",
        "    \"bed\": \"Hospital Facilities\",\n",
        "\n",
        "    # General Experience\n",
        "    \"experience\": \"Overall Experience\",\n",
        "    \"service\": \"Overall Experience\",\n",
        "    \"care\": \"Overall Experience\"\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# Functions\n",
        "# -------------------------------\n",
        "def normalize_topic(topic):\n",
        "    topic = re.sub(r'^\\s*\\d+\\.\\s*', '', topic).strip()\n",
        "    for key, normalized in topic_normalization.items():\n",
        "        if key.lower() in topic.lower():\n",
        "            return normalized\n",
        "    return topic\n",
        "\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "def extract_and_normalize_topics(main_topic_text):\n",
        "    raw_extracted_topics = re.findall(r'^\\s*\\d+\\.\\s*(.+)', main_topic_text, re.MULTILINE)\n",
        "\n",
        "    if not raw_extracted_topics:\n",
        "        lines = [line.strip() for line in main_topic_text.split('\\n') if line.strip()]\n",
        "        raw_extracted_topics = [\n",
        "            re.sub(r'^(Topic\\\\s*\\\\d+\\\\s*:|^\\\\d+\\\\.\\\\s*|^\\\\s*-\\\\s*)*', '', line, flags=re.IGNORECASE).strip()\n",
        "            for line in lines\n",
        "        ]\n",
        "        raw_extracted_topics = [t for t in raw_extracted_topics if t]\n",
        "\n",
        "    normalized_topics_list = []\n",
        "    for topic_text in raw_extracted_topics:\n",
        "        normalized_topic = normalize_topic(topic_text)\n",
        "        if normalized_topic:\n",
        "            normalized_topics_list.append(normalized_topic)\n",
        "\n",
        "    return normalized_topics_list[:3]  # Top 3\n",
        "\n",
        "# -------------------------------\n",
        "# Load pipeline for text generation\n",
        "# -------------------------------\n",
        "text_gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/falcon-7b-instruct\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Prepare data\n",
        "# -------------------------------\n",
        "# Assuming 'google_neg' DataFrame exists with a 'Comment' column\n",
        "english_reviews = google_neg[google_neg['Comment'].apply(is_english)]\n",
        "subset_size = 50\n",
        "google_subset = english_reviews.iloc[:subset_size].copy()\n",
        "\n",
        "google_prompts = [\n",
        "    f\"In the following customer review, pick out the main 3 topics. Return them in a numbered list format, each on a new line.\\n\\nReview: {review}\\nMain topics:\"\n",
        "    for review in google_subset['Comment']\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# Generate topics using pipeline\n",
        "# -------------------------------\n",
        "google_results = []\n",
        "for prompt in tqdm(google_prompts, desc=\"Generating topics\"):\n",
        "    result = text_gen(prompt, max_new_tokens=64, temperature=0.7)\n",
        "    google_results.append(result[0]['generated_text'])\n",
        "\n",
        "# -------------------------------\n",
        "# Extract and normalize topics\n",
        "# -------------------------------\n",
        "google_subset['main_topic_raw'] = [\n",
        "    text.split(\"Main topics:\")[-1].strip() for text in google_results\n",
        "]\n",
        "google_subset['topics_list'] = google_subset['main_topic_raw'].apply(extract_and_normalize_topics)\n",
        "\n",
        "# -------------------------------\n",
        "# Count topics\n",
        "# -------------------------------\n",
        "all_topics = [topic for sublist in google_subset['topics_list'] for topic in sublist]\n",
        "topic_counts = Counter(all_topics)\n",
        "top_3_overall_topics = topic_counts.most_common(3)\n",
        "\n",
        "# -------------------------------\n",
        "# Display results\n",
        "# -------------------------------\n",
        "print(\"---\")\n",
        "print(\"## Overall Top 3 Most Common Topics\")\n",
        "for topic, count in top_3_overall_topics:\n",
        "    print(f\"- **{topic}**: {count} reviews\")\n",
        "\n",
        "print(\"\\n---\")\n",
        "print(\"## Reviews with Extracted and Limited Topics\")\n",
        "for index, row in google_subset.iterrows():\n",
        "    print(f\"**Review {index+1}:** {row['Comment']}\")\n",
        "    print(\"  **Extracted Topics (Max 3):**\")\n",
        "    if row['topics_list']:\n",
        "        for i, topic in enumerate(row['topics_list']):\n",
        "            print(f\"  {i+1}. {topic}\")\n",
        "    else:\n",
        "        print(\"  No main topics extracted.\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "hAcFQJFR_EY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The output of the model will be the top 3 topics from each review. Append each of these topics from each review to create a comprehensive list.\n",
        "\n",
        "all_topics = [topic for sublist in google_subset['topics_list'] for topic in sublist]\n",
        "print(all_topics[:15]) # Adjust 15 to any number you prefer, or remove [:15] for the whole list"
      ],
      "metadata": {
        "id": "iX41wRWtLbpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this list as input to run BERTopic again.\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "bertopic_model = BERTopic(language=\"english\", verbose=True,\n",
        "                          min_topic_size=5, nr_topics=\"auto\", n_gram_range=(1, 2))\n",
        "\n",
        "# Fit BERTopic on your 'all_topics' list\n",
        "# The 'meta_topics' output here refers to the assigned meta-topic ID for each item in 'all_topics'\n",
        "meta_topics, probabilities = bertopic_model.fit_transform(all_topics)\n",
        "\n",
        "# Get information about the generated meta-topics\n",
        "meta_topic_info = bertopic_model.get_topic_info()\n",
        "\n",
        "print(meta_topic_info)\n",
        "\n",
        "print(\"\\n--- Top words for each BERTopic Meta-Topic ---\")\n",
        "for topic_id in meta_topic_info['Topic'].unique():\n",
        "    if topic_id != -1: # -1 usually represents outliers/noise\n",
        "        print(f\"\\nMeta-Topic {topic_id}: {bertopic_model.get_topic(topic_id)}\")"
      ],
      "metadata": {
        "id": "Su0Jr8YiOW4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the top 5 topics as a bar chart\n",
        "fig = bertopic_model.visualize_barchart(top_n_topics=5)\n",
        "\n",
        "# Adjust the figure size\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width= 1400,  # Adjust width for more space\n",
        "    height=600  # Adjust height for more space\n",
        ")\n",
        "\n",
        "# Rotate x-axis labels to prevent overlap\n",
        "fig.update_layout(\n",
        "    xaxis_tickangle=-45  # Rotate labels by 45 degrees to avoid overlap\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QToUdv-McdUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topics with a heatmap\n",
        "bertopic_model.visualize_heatmap()"
      ],
      "metadata": {
        "id": "klyEp99OdZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/falcon-7b-instruct\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "if not all_topics:\n",
        "    print(\"The 'all_topics' list is empty. Cannot generate insights.\")\n",
        "else:\n",
        "    print(\"\\nGenerating Actionable Insights from all_topics\")\n",
        "\n",
        "    # Join the topics into a single string for the prompt\n",
        "    topics_for_insight_prompt = \", \".join(all_topics)\n",
        "\n",
        "    # Construct the full prompt as specified\n",
        "    actionable_insight_prompt = (\n",
        "        \"For the following text topics obtained from negative customer reviews, \"\n",
        "        \"can you give some actionable insights that would help this gym company?\\n\\n\"\n",
        "        f\"Topics: {topics_for_insight_prompt}\\n\\n\"\n",
        "        \"Actionable Insights:\" # This phrase helps guide the LLM's output\n",
        "    )\n",
        "\n",
        "    print(f\"Prompt sent to Falcon model (truncated for display):\\n{actionable_insight_prompt[:500]}...\\n\")\n",
        "\n",
        "    # Run the Falcon-7b-Instruct model to generate insights\n",
        "    # Adjust max_new_tokens for the desired length of insights.\n",
        "    # Adjust temperature for creativity (higher = more creative).\n",
        "\n",
        "    insight_results = generator(\n",
        "        actionable_insight_prompt,\n",
        "        max_new_tokens=300, # Example: 300 tokens for comprehensive insights\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Extract the generated insights from the model's output\n",
        "    generated_insights = insight_results[0]['generated_text'].split(\"Actionable Insights:\")[-1].strip()\n",
        "\n",
        "    print(\"\\nGenerated Actionable Insights from Falcon Model\")\n",
        "    print(generated_insights)"
      ],
      "metadata": {
        "id": "f9wrjlUlrCRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim LDA Comparison\n",
        "\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "0nqBgqDGspq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define paths to save the pickle files in your Google Drive\n",
        "google_neg_path = '/content/drive/My Drive/Colab Notebooks/google_neg_reviews.pkl'\n",
        "trustpilot_neg_path = '/content/drive/My Drive/Colab Notebooks/trustpilot_neg_reviews.pkl'\n",
        "\n",
        "# Load (unpickle) the DataFrames\n",
        "with open(google_neg_path, 'rb') as f:\n",
        "    google_neg = pickle.load(f)\n",
        "\n",
        "with open(trustpilot_neg_path, 'rb') as f:\n",
        "    trustpilot_neg = pickle.load(f)"
      ],
      "metadata": {
        "id": "bWrUAteVKIJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the first few rows from both DataFrames\n",
        "print(google_neg['clean_tokens'].head())\n",
        "print(trustpilot_neg['clean_tokens'].head())"
      ],
      "metadata": {
        "id": "qLi7POHquBOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the preprocessing required to run the LDA model from Gensim. Use the list of negative reviews (combined Google and Trustpilot reviews).\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))  # List of stopwords in English\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocess each review (already tokenized)\n",
        "def preprocess(tokens):\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to all Google reviews\n",
        "google_preprocessed = [preprocess(tokens) for tokens in google_neg['clean_tokens'].tolist()]\n",
        "\n",
        "# Apply preprocessing to all Trustpilot reviews\n",
        "trustpilot_preprocessed = [preprocess(tokens) for tokens in trustpilot_neg['clean_tokens'].tolist()]\n",
        "\n",
        "# Combine the preprocessed Google and Trustpilot reviews\n",
        "preprocessed_texts = google_preprocessed + trustpilot_preprocessed\n",
        "\n",
        "# Remove rare words (appear in fewer than 3 documents)\n",
        "# Create dictionary\n",
        "id2word = corpora.Dictionary(preprocessed_texts)\n",
        "\n",
        "# Filter tokens that appear in fewer than 3 documents or more than 50% of the documents\n",
        "id2word.filter_extremes(no_below=3, no_above=0.5)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [id2word.doc2bow(text) for text in preprocessed_texts]"
      ],
      "metadata": {
        "id": "ZeuFZwussgc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Gensim, perform LDA on the tokenised data. Specify the number of topics = 10.\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "import numpy as np\n",
        "\n",
        "# Number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Train the LDA model using the preprocessed corpus and dictionary\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=15,  # Number of passes over the entire corpus (higher = better)\n",
        "    iterations=400,  # Number of iterations for each pass\n",
        "    eval_every=None  # Disable evaluation during training to save time\n",
        ")\n",
        "\n",
        "# Display the topics and their top words\n",
        "topics = lda_model.print_topics(num_words=10)  # Show top 10 words for each topic\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "pswucb-lvHUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "ZfgVDUDwv76_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the visualisations of the topics, displaying the distance maps and the bar chart listing out the most salient terms.\n",
        "\n",
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualizing the LDA topics using pyLDAvis\n",
        "lda_visualization = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
        "\n",
        "# Display the visualization\n",
        "pyLDAvis.display(lda_visualization)"
      ],
      "metadata": {
        "id": "XR6FpxuLvxxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}